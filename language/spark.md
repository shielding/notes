### scala语言

* 函数式编程更有利于多cpu并发（python并发性不好，不适合企业级大规模数据分析）
* 类似java，运行在jvm上，（scala的作者就是javac作者）但比java简洁很多
* hadoop就是java开发的，兼容；
* 每个值都是对象
    * eg. 1 to 5 等价于 1.to(5) 
* 匿名函数（lamda表达式）
* 参数可以是函数
* 具体的容器类继承了各种特质(trait)
* case类会自动生成伴生对象（以及apply、unapply）
* 函数类型： （参数类型）=>返回值类型
* 函数值（函数字面量）： （参数）=>{函数体}
* spark 函数体中最后一个出现的值是返回值
* def 函数名（参数名：参数类型）：返回值类型 = ｛函数体｝
* 如果没有参数，可以省略（）括号，但是调用的时候也不能加（）调用了。
函数体只有一个值时可以省略｛｝。
* :Unit= 可以一起删除。
* 可以推断出来的返回值类型可以省略，比如 :Int
* 和java不同，scala的主构造器可以有参数，并且这个参数前可以加val或var，如果加了这俩中的一个，该参数自动成为成员变量。否则就只是传参。
更新：类.name_=（“New”）或者 类.name=“New”

辅助构造器一定要调用之前的主构造器或者辅助构造器。就像是连环调用，重用了代码。

- [ ] 为什么需要构造器？完成初始化还是？

Object 单例对象：所有字段和方法都是静态的。不需要实例化。
单例对象分为伴生（有同名的类）和孤立。伴生单例对象和类之间可以互相访问数据。

Array不需要new一个是因为自动调用了Array的apply方法来生成对象。
触发apply：通过（）括号的方式给类实例或单例对象名一个或多个参数时。
scala会自动调用伴生对象中的apply（自己写），这样类就可以当函数用了。函数也可以当类用，.apply（已经自动写好了）即可。这样就兼顾了面向对象和函数式编程。

- [ ] 总结：spark的伴生单例对象相当于就是c++里把和某一个类相关的静态数据和函数集合到一起。这样既有结构也省得声明friend就可以互相访问private的东西了。

抽象类前要加abstract，在其中定义的抽象字段和函数不用加abstract。抽象字段必须标明类型。
继承（extends）父类时，如果重写的是父类中不抽象的，需要加override。
char、int等都是继承自AnyVal，保存在寄存器中。不可以new。
引用类型继承自AnyRef，保存在堆中。可以new。
如果返回结果为空，不建议用null。建议将返回值类型设为option类，这样结果如果不为空，会封装成Some对象返回；如果为空，会返回None。如果不想让返回的是None，可以用getOrElse。

trait不仅能实现java中接口的作用，还可以定义具体而非抽象的函数方法。混入特质（英文为mixin，比如extends或者with）可以实现类似多重继承。

x=readChar（）还可以是readLine（）
x match｛
         case “A”=>
         case _=>
｝



### spark
* YARN可以为spark分配资源
* hadoop的计算框架MapReduce缺点：抽象性高，只有两个函数，但也因此有表达的局限性；；必须所有map结束才能开始reduce，需要等，而且磁盘IO开销太大
* spark基于有向无环图DAG，迭代计算时在内存完成，不用存取磁盘；流水线优化


### RDD运行原理
* RDD具有天然的容错性，因为DAG中就记录了lineage亲缘关系。传统的数据备份、日志在大数据这里都不现实。
* 中间结果存到内存，既节省了磁盘IO的时间，还节省了序列化/反序列化的时间。（序列化就是把数据变成可以传输和存储的格式）
* 如何分段？shuffle（涉及网络中大量数据转发）的一定是宽依赖。窄依赖可以分到一个段里。
* 窄依赖可以流水线优化，fork/join机制（并行执行任务的框架）
* 宽依赖不行是因为 shuffle一定涉及磁盘IO
* 分到一个阶段的可以流水线优化


### spark部署
* YARN、mesos比自带的standalone性能好